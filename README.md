# Proyecto ETL - Contrataciones PÃºblicas de Mendoza

Este proyecto implementa un pipeline ETL utilizando Apache Airflow para descargar, transformar y cargar datos abiertos de contrataciones pÃºblicas de la provincia de Mendoza (Argentina), conforme al estÃ¡ndar OCDS (Open Contracting Data Standard).

ğŸ“Š Dataset
Los datos provienen de fuentes pÃºblicas del Gobierno de Mendoza y estÃ¡n estructurados bajo el estÃ¡ndar OCDS.
[Datasets](https://datosabiertos-compras.mendoza.gov.ar/datasets/)

âœ… Resultado
Este pipeline permite mantener una base de datos actualizada y limpia con informaciÃ³n clave sobre procesos de licitaciÃ³n, adjudicaciones y contratos.

## ğŸ“Œ Objetivo

Transformar, limpiar y cargar datos abiertos sobre contrataciones pÃºblicas en una base de datos MySQL para su anÃ¡lisis posterior.


## ğŸ› ï¸ Herramientas utilizadas

- **Apache Airflow**: OrquestaciÃ³n de tareas y ejecuciÃ³n programada del flujo ETL.
- **Python 3**: Desarrollo de scripts personalizados.
- **pandas**: ManipulaciÃ³n y transformaciÃ³n de datos.
- **SQLAlchemy + PyMySQL**: ConexiÃ³n e inserciÃ³n de datos en base de datos MySQL.
- **MySQL**: Base de datos relacional donde se almacenan los datos procesados.
- **OCDS**: EstÃ¡ndar para estructuraciÃ³n de datos de contrataciones pÃºblicas.

## ğŸ“ Estructura del proyecto

```text
airflow_etl_mendoza/
+-- dags/
Â¦   +-- etl_mendoza.py              # DAG principal
Â¦   +-- scripts/
Â¦       +-- cargar_mysql.py
Â¦       +-- guardar_csvs.py
Â¦       +-- transformador.py
+-- data/
Â¦   +-- procesos.csv
Â¦   +-- adjudicaciones.csv
Â¦   +-- contratos.csv
+-- tmp/                            # Archivos temporales .pkl
+-- requirements.txt
+-- docker-compose.yml              
+-- .gitignore
+-- README.md
```




## ğŸ“Š Tablas principales en la base de datos

- `procesos`: Contiene metadatos generales del proceso de contrataciÃ³n.
- `adjudicaciones`: InformaciÃ³n sobre adjudicaciones realizadas.
- `contratos`: Detalles de contratos firmados.


ğŸš§ Docker Compose

El archivo docker-compose.yml incluye contenedores para PostgreSQL (usado por Airflow) y servicios de Airflow Webserver y Scheduler. Airflow se comunica con una base de datos MySQL externa configurada en los scripts.

## ğŸš€ CÃ³mo ejecutar

1. Instalar dependencias:
   ```bash
   pip install -r requirements.txt
2. Configurar la conexiÃ³n a MySQL en cargar_mysql.py
3. Ejecutar Airflow:
   ```bash
      airflow db init
      airflow webserver
      airflow scheduler
â–¶ï¸ EjecuciÃ³n Local

   git clone https://github.com/Leonardorf/airflow_etl_mendoza.git

   cd airflow_etl_mendoza

   docker-compose up

Luego accedÃ© a Airflow en http://localhost:8080, activÃ¡ el DAG etl_mendoza y ejecÃ»talo manualmente o programÃ¡ su ejecuciÃ³n periÃ³dica.
Este proyecto se ejecuta desde el DAG etl_contrataciones_mendoza en el entorno de Airflow. Las tareas incluyen:

Descarga de datos (guardar_csvs)

TransformaciÃ³n (transformador)

Carga en MySQL (cargar_mysql

ğŸ“Š Dataset

Los datos provienen de fuentes pÃºblicas del Gobierno de Mendoza y estÃ¡n estructurados bajo el estÃ¡ndar OCDS. Incluyen procesos de contrataciÃ³n, adjudicaciones y contratos.

âœ… Resultado

Este pipeline permite mantener una base de datos estructurada y actualizada con informaciÃ³n clave sobre licitaciones, ideal para anÃ¡lisis de transparencia, eficiencia del gasto pÃºblico y oportunidades de mejora.


![Imagen de ejemplo](airflow_dags.png)

![Imagen de ejemplo](mysql.png)



## ğŸ“Œ Licencia
MIT
